<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Apache Spark</title><link rel="stylesheet" type="text/css" href="eclipse_book.css"><meta name="generator" content="DocBook XSL Stylesheets V1.77.1"><meta name="keywords" content="ESB, Apache ServiceMix, Open Source, open source, integration, OSGi, enterprise service bus, Apache ServiceMix documentation, Apache Karaf, Red Hat JBoss Fuse, Red Hat JBoss Fuse documentation"><meta name="keywords" content="ESB, Apache ServiceMix, Open Source, open source, integration, OSGi, enterprise service bus, Apache ServiceMix documentation, Apache Karaf, Red Hat JBoss Fuse, Red Hat JBoss Fuse documentation"><link rel="home" href="index.html" title="Red Hat JBoss Fuse Tooling for Eclipse"><link rel="up" href="IDU-Components.html" title="Apache Camel Component Reference"><link rel="prev" href="IDU-Solr.html" title="Solr"><link rel="next" href="IDU-SparkREST.html" title="Spark REST"><link rel="copyright" href="tmdisclaim.html" title="Trademark Disclaimer"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a name="IDU-ApacheSpark"></a>Apache Spark</h1></div></div></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162362"></a>Apache Spark component</h2></div></div></div><p>This documentation page covers the <a class="link" href="http://spark.apache.org/" target="_top">Apache Spark</a> component for the Apache Camel. The main purpose of the Spark integration with Camel is to provide a bridge between Camel connectors and Spark tasks. In particular Camel connector provides a way to route message from various transports, dynamically choose a task to execute, use incoming message as input data for that task and finally deliver the results of the execution back to the Camel pipeline.</p></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162370"></a>Supported architectural styles</h2></div></div></div><p>Spark component can be used as a driver application deployed into an application server (or executed as a fat jar).</p><div class="mediaobject"><img src="images/spark_01.png" width="1936.8999999999999"></div><p>Spark component can also be submitted as a job directly into the Spark cluster.</p><div class="mediaobject"><img src="images/spark_02.png" width="625.2"></div><p>While Spark component is primary designed to work as a <span class="emphasis"><em>long running job</em></span> serving as an bridge between Spark cluster and the other endpoints, you can also use it as a <span class="emphasis"><em>fire-once</em></span> short job.</p></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162391"></a>Running Spark in OSGi servers</h2></div></div></div><p>Currently the Spark component doesn't support execution in the OSGi container. Spark has been designed to be executed as a fat jar, usually submitted as a job to a cluster. For those reasons running Spark in an OSGi server is at least challenging and is not support by Camel as well.</p></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162396"></a>URI format</h2></div></div></div><p>Currently the Spark component supports only producers - it it intended to invoke a Spark job and return results. You can call RDD, data frame or Hive SQL job.</p><pre class="programlisting">spark:{rdd|dataframe|hive}</pre></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162403"></a>RDD jobs </h2></div></div></div><p>To invoke an RDD job, use the following URI:</p><pre class="programlisting">spark:rdd?rdd=#testFileRdd&amp;rddCallback=#transformation</pre><p> Where <code class="code">rdd</code> option refers to the name of an RDD instance (subclass of <code class="code">org.apache.spark.api.java.JavaRDDLike</code>) from a Camel registry, while <code class="code">rddCallback</code> refers to the implementation of <code class="code">org.apache.camel.component.spark.RddCallback</code> interface (also from a registry). RDD callback provides a single method used to apply incoming messages against the given RDD. Results of callback computations are saved as a body to an exchange.</p><pre class="programlisting">public interface RddCallback&lt;T&gt; {
    T onRdd(JavaRDDLike rdd, Object... payloads);
}</pre><p>The following snippet demonstrates how to send message as an input to the job and return results:</p><pre class="programlisting">String pattern = "job input";
long linesCount = producerTemplate.requestBody("spark:rdd?rdd=#myRdd&amp;rddCallback=#countLinesContaining", pattern, long.class);</pre><p>The RDD callback for the snippet above registered as Spring bean could look as follows:</p><pre class="programlisting">@Bean
RddCallback&lt;Long&gt; countLinesContaining() {
    return new RddCallback&lt;Long&gt;() {
        Long onRdd(JavaRDDLike rdd, Object... payloads) {
            String pattern = (String) payloads[0];
            return rdd.filter({line -&gt; line.contains(pattern)}).count();
        }
    }
}</pre><p>The RDD definition in Spring could looks as follows:</p><pre class="programlisting">@Bean
JavaRDDLike myRdd(JavaSparkContext sparkContext) {
  return sparkContext.textFile("testrdd.txt");
}</pre></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162438"></a>RDD jobs options</h2></div></div></div><table id="d0e162441"><tbody><tr><th>Option</th><th>Description</th><th>Default value</th></tr><tr><td><code class="code">rdd</code></td><td>RDD instance (subclass of <code class="code">org.apache.spark.api.java.JavaRDDLike</code>).</td><td><code class="code">null</code></td></tr><tr><td><code class="code">rddCallback</code></td><td>Instance of <code class="code">org.apache.camel.component.spark.RddCallback</code> interface.</td><td><code class="code">null</code></td></tr></tbody></table></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162474"></a>Void RDD callbacks</h2></div></div></div><p>If your RDD callback doesn't return any value back to a Camel pipeline, you can either return <code class="code">null</code> value or use <code class="code">VoidRddCallback</code> base class:</p><pre class="programlisting">@Bean
RddCallback&lt;Void&gt; rddCallback() {
  return new VoidRddCallback() {
        @Override
        public void doOnRdd(JavaRDDLike rdd, Object... payloads) {
            rdd.saveAsTextFile(output.getAbsolutePath());
        }
    };
}</pre></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162487"></a>Converting RDD callbacks</h2></div></div></div><p>If you know what type of the input data will be sent to the RDD callback, you can use <code class="code">ConvertingRddCallback</code> and let Camel to automatically convert incoming messages before inserting those into the callback:</p><pre class="programlisting">@Bean
RddCallback&lt;Long&gt; rddCallback(CamelContext context) {
  return new ConvertingRddCallback&lt;Long&gt;(context, int.class, int.class) {
            @Override
            public Long doOnRdd(JavaRDDLike rdd, Object... payloads) {
                return rdd.count() * (int) payloads[0] * (int) payloads[1];
            }
        };
    };
}</pre></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162497"></a>Annotated RDD callbacks</h2></div></div></div><p>Probably the easiest way to work with the RDD callbacks is to provide class with method marked with <code class="code">@RddCallback</code> annotation:</p><pre class="programlisting">import static org.apache.camel.component.spark.annotations.AnnotatedRddCallback.annotatedRddCallback;
 
@Bean
RddCallback&lt;Long&gt; rddCallback() {
    return annotatedRddCallback(new MyTransformation());
}
 
...
 
import org.apache.camel.component.spark.annotation.RddCallback;
 
public class MyTransformation {
 
    @RddCallback
    long countLines(JavaRDD&lt;String&gt; textFile, int first, int second) {
        return textFile.count() * first * second;
    }
 
}</pre><p>If you will pass CamelContext to the annotated RDD callback factory method, the created callback will be able to convert incoming payloads to match the parameters of the annotated method:</p><pre class="programlisting">import static org.apache.camel.component.spark.annotations.AnnotatedRddCallback.annotatedRddCallback;
 
@Bean
RddCallback&lt;Long&gt; rddCallback(CamelContext camelContext) {
    return annotatedRddCallback(new MyTransformation(), camelContext);
}
 
...

 
import org.apache.camel.component.spark.annotation.RddCallback;
 
public class MyTransformation {
 
    @RddCallback
    long countLines(JavaRDD&lt;String&gt; textFile, int first, int second) {
        return textFile.count() * first * second;
    }
 
}
 
...
 
// Convert String "10" to integer
long result = producerTemplate.requestBody("spark:rdd?rdd=#rdd&amp;rddCallback=#rddCallback" Arrays.asList(10, "10"), long.class);</pre></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162511"></a>DataFrame jobs</h2></div></div></div><p> </p><p>Instead of working with RDDs Spark component can work with DataFrames as well. </p><p>To invoke an DataFrame job, use the following URI:</p><pre class="programlisting">spark:dataframe?dataFrame=#testDataFrame&amp;dataFrameCallback=#transformation</pre><p> Where <code class="code">dataFrame</code> option refers to the name of a DataFrame instance (instance of <code class="code">org.apache.spark.sql.DataFrame</code>) from a Camel registry, while <code class="code">dataFrameCallback</code> refers to the implementation of <code class="code">org.apache.camel.component.spark.DataFrameCallback</code> interface (also from a registry). DataFrame callback provides a single method used to apply incoming messages against the given DataFrame. Results of callback computations are saved as a body to an exchange.</p><pre class="programlisting">public interface DataFrameCallback&lt;T&gt; {
    T onDataFrame(DataFrame dataFrame, Object... payloads);
}</pre><p>The following snippet demonstrates how to send message as an input to a job and return results:</p><pre class="programlisting">String model = "Micra";
long linesCount = producerTemplate.requestBody("spark:dataFrame?dataFrame=#cars&amp;dataFrameCallback=#findCarWithModel", model, long.class);</pre><p>The DataFrame callback for the snippet above registered as Spring bean could look as follows:</p><pre class="programlisting">@Bean
RddCallback&lt;Long&gt; findCarWithModel() {
    return new DataFrameCallback&lt;Long&gt;() {
    	@Override
    	public Long onDataFrame(DataFrame dataFrame, Object... payloads) {
        	String model = (String) payloads[0];
        	return dataFrame.where(dataFrame.col("model").eqNullSafe(model)).count();
    	}
	};
}</pre><p>The DataFrame definition in Spring could looks as follows:</p><pre class="programlisting">@Bean
DataFrame cars(HiveContext hiveContext) {
  	DataFrame jsonCars = hiveContext.read().json("/var/data/cars.json");
 	jsonCars.registerTempTable("cars");
	return jsonCars;
}</pre></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162550"></a>DataFrame jobs options</h2></div></div></div><table id="d0e162553"><tbody><tr><th>Option</th><th>Description</th><th>Default value</th></tr><tr><td><code class="code">dataFrame</code></td><td>DataFrame instance (subclass of <code class="code">org.apache.spark.sql.DataFrame</code>).</td><td><code class="code">null</code></td></tr><tr><td><code class="code">dataFrameCallback</code></td><td>Instance of <code class="code">org.apache.camel.component.spark.DataFrameCallback</code> interface.</td><td><code class="code">null</code></td></tr></tbody></table><p> </p></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162588"></a>Hive jobs</h2></div></div></div><p> Instead of working with RDDs or DataFrame Spark component can also receive Hive SQL queries as payloads. To send Hive query to Spark component, use the following URI:</p><pre class="programlisting">spark:hive</pre><p>The following snippet demonstrates how to send message as an input to a job and return results:</p><pre class="programlisting">long carsCount = template.requestBody("spark:hive?collect=false", "SELECT * FROM cars", Long.class);
List&lt;Row&gt; cars = template.requestBody("spark:hive", "SELECT * FROM cars", List.class);</pre><p>The table we want to execute query against should be registered in a HiveContext before we query it. For example in Spring such registration could look as follows:</p><pre class="programlisting">@Bean
DataFrame cars(HiveContext hiveContext) {
  	DataFrame jsonCars = hiveContext.read().json("/var/data/cars.json");
 	jsonCars.registerTempTable("cars");
	return jsonCars;
}</pre></div><div class="simplesect"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d0e162603"></a>Hive jobs options</h2></div></div></div><table id="d0e162606"><tbody><tr><th>Option</th><th>Description</th><th>Default value</th></tr><tr><td><code class="code">collect</code></td><td>Indicates if results should be collected (as a list of <code class="code">org.apache.spark.sql.Row</code> instances) or if <code class="code">count()</code> should be called against those.</td><td><code class="code">true</code></td></tr></tbody></table></div></div></body></html>